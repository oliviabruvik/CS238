{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "path = './'\n",
    "\n",
    "## read in example.gph\n",
    "def read_in_data(filename, path):\n",
    "    filename_path = path + filename + '.csv'\n",
    "    with open(filename_path, 'r') as f:\n",
    "      # data = np.loadtxt(filename_path, delimiter=',')\n",
    "      # header = data[0]\n",
    "      # df = data[1:]\n",
    "      df = pd.read_csv(filename_path, header=0)\n",
    "      header = df.columns\n",
    "      # df = df.to_numpy()\n",
    "    return df, header\n",
    "\n",
    "df, header = read_in_data(\"lice_data\", path)\n",
    "lice_data = df[['week',\n",
    "               \"sea_temp\",\n",
    "               \"adult_femalelice\",\n",
    "               'active_cleanerfirsh',\n",
    "               'bath_treatment',\n",
    "               'feed_treatment',\n",
    "               'mechanical_removal',\n",
    "               'release_of_cleanerfish',\n",
    "               'locality_number',\n",
    "                'date',\n",
    "                'year']]\n",
    "\n",
    "dp_round = 2\n",
    "lice_data = lice_data.round(dp_round)\n",
    "lice_data = lice_data.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model-Free Q-Learning with 8 Possible Actions:\n",
    "\n",
    "- 0: no treatment\n",
    "- 1: bath only\n",
    "- 2: feed only\n",
    "- 3: mechanical only\n",
    "- 4: bath + feed\n",
    "- 5: bath + mechanical\n",
    "- 6: feed + mechanical\n",
    "- 7: bath + feed + mechanical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class QLearning:\n",
    "    def __init__(self, num_states, num_actions, discount, Q=None, alpha=0.5):\n",
    "        self.num_states = num_states\n",
    "        self.num_actions = num_actions\n",
    "        self.discount = discount\n",
    "        self.alpha = alpha\n",
    "        if Q is None or Q.size == 0:\n",
    "            self.Q = np.zeros((num_states, num_actions))\n",
    "        else:\n",
    "            self.Q = Q\n",
    "\n",
    "def get_state_idx(lice_level, min_lice, dp_round):\n",
    "    return int((lice_level - min_lice) * (10 ** dp_round))\n",
    "\n",
    "def get_action(model, state_idx, eps):\n",
    "    if np.random.random() < eps:\n",
    "        return np.random.randint(0, model.num_actions)\n",
    "    else:\n",
    "        return np.argmax(model.Q[state_idx])\n",
    "\n",
    "def get_action_from_data(row):\n",
    "    \"\"\"Convert treatment combinations to action index\n",
    "    0: no treatment\n",
    "    1: bath only\n",
    "    2: feed only\n",
    "    3: mechanical only\n",
    "    4: bath + feed\n",
    "    5: bath + mechanical\n",
    "    6: feed + mechanical\n",
    "    7: bath + feed + mechanical\n",
    "    \"\"\"\n",
    "    bath = row['bath_treatment']\n",
    "    feed = row['feed_treatment']\n",
    "    mech = row['mechanical_removal']\n",
    "    \n",
    "    if bath and feed and mech:\n",
    "        return 7\n",
    "    elif feed and mech:\n",
    "        return 6\n",
    "    elif bath and mech:\n",
    "        return 5\n",
    "    elif bath and feed:\n",
    "        return 4\n",
    "    elif mech:\n",
    "        return 3\n",
    "    elif feed:\n",
    "        return 2\n",
    "    elif bath:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0  \n",
    "\n",
    "def get_action_cost(action):\n",
    "    \"\"\"Return cost of each action combination\"\"\"\n",
    "    costs = {\n",
    "        0: 0.0,    # no treatment\n",
    "        1: 1.0,    # bath only\n",
    "        2: 1.5,    # feed only\n",
    "        3: 2.0,    # mechanical only\n",
    "        4: 2.5,    # bath + feed\n",
    "        5: 3.0,    # bath + mechanical\n",
    "        6: 3.5,    # feed + mechanical\n",
    "        7: 4.5     # bath + feed + mechanical\n",
    "    }\n",
    "    return costs[action]\n",
    "\n",
    "def calculate_reward(current_lice, next_lice, action, beta1, beta2, threshold=0.5):\n",
    "    \"\"\"reward function with treatment costs\"\"\"\n",
    "    # penalty for current lice level\n",
    "    base_penalty = -beta1 * current_lice\n",
    "    \n",
    "    # HUGE penalty for exceeding lice level 0.5\n",
    "    threshold_penalty = -10 * beta1 * max(0, current_lice - threshold)\n",
    "    \n",
    "    # delta lice level \n",
    "    reduction_reward = 5 * beta1 * max(0, current_lice - next_lice)\n",
    "    \n",
    "    # cost\n",
    "    treatment_cost = -beta2 * get_action_cost(action)\n",
    "    \n",
    "    return base_penalty + threshold_penalty + reduction_reward + treatment_cost\n",
    "\n",
    "def update(model, s, a, r, s2, model_large):\n",
    "    \"\"\"Update Q-values using Q-learning update rule\"\"\"\n",
    "    if 0 <= s < model.num_states and 0 <= s2 < model.num_states and 0 <= a < model.num_actions:\n",
    "        model.Q[s, a] += model.alpha * (r + model.discount * np.max(model.Q[s2, :]) - model.Q[s, a])\n",
    "    return model\n",
    "\n",
    "def simulate_episode(model, sublice_data, eps, beta1, beta2, is_training=True, dp_round=2):\n",
    "    \"\"\"Run one episode of simulation\"\"\"\n",
    "    rewards = []\n",
    "    min_lice = sublice_data['adult_femalelice'].min()\n",
    "    \n",
    "    for i in range(len(sublice_data) - 1):\n",
    "        current_lice = sublice_data.loc[i, 'adult_femalelice']\n",
    "        next_lice = sublice_data.loc[i + 1, 'adult_femalelice']\n",
    "        \n",
    "        state_idx = get_state_idx(current_lice, min_lice, dp_round)\n",
    "        next_state_idx = get_state_idx(next_lice, min_lice, dp_round)\n",
    "        \n",
    "        if is_training:\n",
    "            action = get_action(model, state_idx, eps)\n",
    "        else:\n",
    "            action = np.random.randint(0, model.num_actions)\n",
    "        \n",
    "        reward = calculate_reward(current_lice, next_lice, action, beta1, beta2)\n",
    "        rewards.append(reward)\n",
    "        \n",
    "        if is_training:\n",
    "            model = update(model, state_idx, action, reward, next_state_idx, True)\n",
    "    \n",
    "    return model, sum(rewards)\n",
    "\n",
    "def run_experiment(model, epochs, locations, lice_data, eps, beta1, beta2, dp_round, num_simulations=3):\n",
    "    \"\"\"Run multiple simulations comparing Q-learning and random policies\"\"\"\n",
    "    q_learning_results = np.zeros((num_simulations, epochs))\n",
    "    random_results = np.zeros((num_simulations, epochs))\n",
    "    \n",
    "    for sim in range(num_simulations):\n",
    "        model.Q = np.zeros((model.num_states, model.num_actions))\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            q_learning_epoch_reward = 0\n",
    "            random_epoch_reward = 0\n",
    "            \n",
    "            for location in locations:\n",
    "                sublice_data = lice_data[lice_data['locality_number'] == location].reset_index(drop=True)\n",
    "                \n",
    "                if len(sublice_data) > 1:\n",
    "                    model, q_reward = simulate_episode(model, sublice_data, eps, beta1, beta2, \n",
    "                                                    is_training=True, dp_round=dp_round)\n",
    "                    q_learning_epoch_reward += q_reward\n",
    "                    \n",
    "                    # random policy \n",
    "                    _, r_reward = simulate_episode(model, sublice_data, eps, beta1, beta2, \n",
    "                                                is_training=False, dp_round=dp_round)\n",
    "                    random_epoch_reward += r_reward\n",
    "            \n",
    "            q_learning_results[sim, epoch] = q_learning_epoch_reward\n",
    "            random_results[sim, epoch] = random_epoch_reward\n",
    "    \n",
    "    return q_learning_results, random_results\n",
    "\n",
    "def plot_comparison(q_learning_results, random_results):\n",
    "    epochs = q_learning_results.shape[1]\n",
    "    epochs_range = np.arange(epochs)\n",
    "    \n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    q_mean = np.mean(q_learning_results, axis=0)\n",
    "    q_std = np.std(q_learning_results, axis=0)\n",
    "    random_mean = np.mean(random_results, axis=0)\n",
    "    random_std = np.std(random_results, axis=0)\n",
    "    \n",
    "    plt.plot(epochs_range, q_mean, 'b-', label='Q-Learning', linewidth=2)\n",
    "    plt.plot(epochs_range, random_mean, 'r--', label='Random Policy', linewidth=2)\n",
    "    plt.fill_between(epochs_range, q_mean - q_std, q_mean + q_std, color='b', alpha=0.2)\n",
    "    plt.fill_between(epochs_range, random_mean - random_std, random_mean + random_std, color='r', alpha=0.2)\n",
    "    \n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Total Reward')\n",
    "    plt.title('Q-Learning vs Random Policy: Total Rewards Over Time')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    print(f\"\\nFinal Statistics (over {len(q_learning_results)} simulations):\")\n",
    "    print(f\"Q-Learning - Final Mean Reward: {q_mean[-1]:.2f} ± {q_std[-1]:.2f}\")\n",
    "    print(f\"Random Policy - Final Mean Reward: {random_mean[-1]:.2f} ± {random_std[-1]:.2f}\")\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "max_lice = max(lice_data['adult_femalelice'])\n",
    "min_lice = min(lice_data['adult_femalelice'])\n",
    "dp_round = 2\n",
    "\n",
    "max_state_idx = get_state_idx(max_lice, min_lice, dp_round)\n",
    "num_states = max_state_idx + 1  # Add 1 because indices start at 0\n",
    "\n",
    "print(f\"State space size: {num_states}\")\n",
    "print(f\"Min lice: {min_lice:.2f}, Max lice: {max_lice:.2f}\")\n",
    "\n",
    "#pParameters\n",
    "epochs = 10\n",
    "alpha = 0.5\n",
    "discount = 0.99\n",
    "beta1 = 5\n",
    "beta2 = 0.5\n",
    "eps = 0.1\n",
    "num_simulations = 3\n",
    "num_actions = 8 \n",
    "\n",
    "# initialize model\n",
    "model = QLearning(num_states, num_actions, discount, alpha=alpha)\n",
    "\n",
    "# experiment\n",
    "locations = set(lice_data['locality_number'])\n",
    "q_learning_results, random_results = run_experiment(\n",
    "    model, epochs, locations, lice_data, \n",
    "    eps, beta1, beta2, dp_round, \n",
    "    num_simulations=num_simulations\n",
    ")\n",
    "\n",
    "plot_comparison(q_learning_results, random_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model-Free Q-Learning with 2 Actions (no treatment, mechanical treatment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class QLearning:\n",
    "    def __init__(self, num_states, num_actions, discount, Q=None, alpha=0.5):\n",
    "        self.num_states = num_states\n",
    "        self.num_actions = num_actions\n",
    "        self.discount = discount\n",
    "        self.alpha = alpha\n",
    "        if Q is None or Q.size == 0:\n",
    "            self.Q = np.zeros((num_states, num_actions))\n",
    "        else:\n",
    "            self.Q = Q\n",
    "\n",
    "def get_state_idx(lice_level, min_lice, dp_round):\n",
    "    return int((lice_level - min_lice) * (10 ** dp_round))\n",
    "\n",
    "def get_action(model, state_idx, eps):\n",
    "    if np.random.random() < eps:\n",
    "        return np.random.randint(0, model.num_actions)\n",
    "    else:\n",
    "        return np.argmax(model.Q[state_idx])\n",
    "\n",
    "def update(model, s, a, r, s2, model_large):\n",
    "    if 0 <= s < model.num_states and 0 <= s2 < model.num_states and 0 <= a < model.num_actions:\n",
    "        model.Q[s, a] += model.alpha * (r + model.discount * np.max(model.Q[s2, :]) - model.Q[s, a])\n",
    "    return model\n",
    "\n",
    "def simulate_episode(model, sublice_data, eps, beta1, beta2, costs, is_training=True, dp_round=2):\n",
    "    rewards = []\n",
    "    min_lice = sublice_data['adult_femalelice'].min()\n",
    "    \n",
    "    for i in range(len(sublice_data) - 1):\n",
    "        current_lice = sublice_data.loc[i, 'adult_femalelice']\n",
    "        next_lice = sublice_data.loc[i + 1, 'adult_femalelice']\n",
    "        \n",
    "        state_idx = get_state_idx(current_lice, min_lice, dp_round)\n",
    "        next_state_idx = get_state_idx(next_lice, min_lice, dp_round)\n",
    "        \n",
    "        if is_training:\n",
    "            action = get_action(model, state_idx, eps)\n",
    "        else:\n",
    "            action = np.random.randint(0, model.num_actions)\n",
    "        \n",
    "        reward = -1 * (beta1 * current_lice + beta2 * costs[bool(action)])\n",
    "        rewards.append(reward)\n",
    "        \n",
    "        if is_training:\n",
    "            model = update(model, state_idx, action, reward, next_state_idx, True)\n",
    "    \n",
    "    return model, sum(rewards)  \n",
    "\n",
    "def run_experiment(model, epochs, locations, lice_data, eps, beta1, beta2, costs, dp_round, num_simulations=3):\n",
    "    q_learning_results = np.zeros((num_simulations, epochs))\n",
    "    random_results = np.zeros((num_simulations, epochs))\n",
    "    \n",
    "    for sim in range(num_simulations):\n",
    "        model.Q = np.zeros((model.num_states, model.num_actions))\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            q_learning_epoch_reward = 0\n",
    "            random_epoch_reward = 0\n",
    "            \n",
    "            for location in locations:\n",
    "                sublice_data = lice_data[lice_data['locality_number'] == location].reset_index(drop=True)\n",
    "                \n",
    "                if len(sublice_data) > 1:\n",
    "                    model, q_reward = simulate_episode(model, sublice_data, eps, beta1, beta2, costs, \n",
    "                                                    is_training=True, dp_round=dp_round)\n",
    "                    q_learning_epoch_reward += q_reward\n",
    "                    \n",
    "                    # random policy episode\n",
    "                    _, r_reward = simulate_episode(model, sublice_data, eps, beta1, beta2, costs, \n",
    "                                                is_training=False, dp_round=dp_round)\n",
    "                    random_epoch_reward += r_reward\n",
    "            \n",
    "            q_learning_results[sim, epoch] = q_learning_epoch_reward\n",
    "            random_results[sim, epoch] = random_epoch_reward\n",
    "    \n",
    "    return q_learning_results, random_results\n",
    "\n",
    "def plot_comparison(q_learning_results, random_results):\n",
    "    epochs = q_learning_results.shape[1]\n",
    "    epochs_range = np.arange(epochs)\n",
    "    \n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    q_mean = np.mean(q_learning_results, axis=0)\n",
    "    q_std = np.std(q_learning_results, axis=0)\n",
    "    random_mean = np.mean(random_results, axis=0)\n",
    "    random_std = np.std(random_results, axis=0)\n",
    "    \n",
    "    # means and confidence intervals\n",
    "    plt.plot(epochs_range, q_mean, 'b-', label='Q-Learning', linewidth=2)\n",
    "    plt.plot(epochs_range, random_mean, 'r--', label='Random Policy', linewidth=2)\n",
    "    plt.fill_between(epochs_range, q_mean - q_std, q_mean + q_std, color='b', alpha=0.2)\n",
    "    plt.fill_between(epochs_range, random_mean - random_std, random_mean + random_std, color='r', alpha=0.2)\n",
    "    \n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Total Reward')\n",
    "    plt.title('Q-Learning vs Random Policy: Total Rewards Over Time')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    print(f\"\\nFinal Statistics (over {len(q_learning_results)} simulations):\")\n",
    "    print(f\"Q-Learning - Final Mean Reward: {q_mean[-1]:.2f} ± {q_std[-1]:.2f}\")\n",
    "    print(f\"Random Policy - Final Mean Reward: {random_mean[-1]:.2f} ± {random_std[-1]:.2f}\")\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "max_lice = max(lice_data['adult_femalelice'])\n",
    "min_lice = min(lice_data['adult_femalelice'])\n",
    "dp_round = 2\n",
    "\n",
    "max_state_idx = get_state_idx(max_lice, min_lice, dp_round)\n",
    "num_states = max_state_idx + 1  # Add 1 because indices start at 0\n",
    "\n",
    "print(f\"State space size: {num_states}\")\n",
    "print(f\"Min lice: {min_lice:.2f}, Max lice: {max_lice:.2f}\")\n",
    "\n",
    "#pParameters\n",
    "epochs = 10\n",
    "alpha = 0.5\n",
    "discount = 0.99\n",
    "beta1 = 5\n",
    "beta2 = 0.5\n",
    "eps = 0.1\n",
    "num_simulations = 3\n",
    "num_actions = 2\n",
    "costs = {True: 1, False: 0}\n",
    "\n",
    "# initialize model\n",
    "model = QLearning(num_states, num_actions, discount, alpha=alpha)\n",
    "\n",
    "# experiment\n",
    "locations = set(lice_data['locality_number'])\n",
    "q_learning_results, random_results = run_experiment(\n",
    "    model, epochs, locations, lice_data, \n",
    "    eps, beta1, beta2, costs, dp_round, \n",
    "    num_simulations=num_simulations\n",
    ")\n",
    "\n",
    "plot_comparison(q_learning_results, random_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ignore me I'm old code!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate proper number of states\n",
    "max_lice = max(lice_data['adult_femalelice'])\n",
    "min_lice = min(lice_data['adult_femalelice'])\n",
    "dp_round = 2\n",
    "\n",
    "# Calculate the maximum possible state index\n",
    "max_state_idx = get_state_idx(max_lice, min_lice, dp_round)\n",
    "num_states = max_state_idx + 1  # Add 1 because indices start at 0\n",
    "\n",
    "print(f\"State space size: {num_states}\")\n",
    "print(f\"Min lice: {min_lice:.2f}, Max lice: {max_lice:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class QLearning:\n",
    "    def __init__(self, num_states, num_actions, discount, Q=None, alpha=0.5):\n",
    "        self.num_states = num_states\n",
    "        self.num_actions = num_actions\n",
    "        self.discount = discount\n",
    "        self.alpha = alpha\n",
    "        if Q is None or Q.size == 0:\n",
    "            self.Q = np.zeros((num_states, num_actions))\n",
    "        else:\n",
    "            self.Q = Q\n",
    "\n",
    "def get_state_idx(lice_level, min_lice, dp_round):\n",
    "    return int((lice_level - min_lice) * (10 ** dp_round))\n",
    "\n",
    "def get_action(model, state_idx, eps):\n",
    "    if np.random.random() < eps:\n",
    "        return np.random.randint(0, model.num_actions)\n",
    "    else:\n",
    "        return np.argmax(model.Q[state_idx])\n",
    "\n",
    "def update(model, s, a, r, s2, model_large):\n",
    "    if 0 <= s < model.num_states and 0 <= s2 < model.num_states and 0 <= a < model.num_actions:\n",
    "        model.Q[s, a] += model.alpha * (r + model.discount * np.max(model.Q[s2, :]) - model.Q[s, a])\n",
    "    return model\n",
    "\n",
    "def calculate_reward(current_lice, next_lice, action, beta1, beta2, costs, threshold=0.5):\n",
    "    \"\"\"reward function that considers:\n",
    "    1. Base penalty for current lice level\n",
    "    2. Extra penalty for exceeding threshold\n",
    "    3. Reward for reducing lice levels\n",
    "    4. Treatment cost penalty\n",
    "    \"\"\"\n",
    "    # base penalty for current lice level\n",
    "    base_penalty = -beta1 * current_lice\n",
    "    \n",
    "    # extra penalty for exceeding threshold\n",
    "    threshold_penalty = -10 * beta1 * max(0, current_lice - threshold)\n",
    "    \n",
    "    # reward for reducing lice levels\n",
    "    reduction_reward = 5 * beta1 * max(0, current_lice - next_lice)\n",
    "    \n",
    "    # treatment cost\n",
    "    treatment_cost = -beta2 * costs[bool(action)]\n",
    "    \n",
    "    return base_penalty + threshold_penalty + reduction_reward + treatment_cost\n",
    "\n",
    "def simulate_episode(model, sublice_data, eps, beta1, beta2, costs, is_training=True, dp_round=2):\n",
    "    rewards = []\n",
    "    min_lice = sublice_data['adult_femalelice'].min()\n",
    "    \n",
    "    for i in range(len(sublice_data) - 1):\n",
    "        current_lice = sublice_data.loc[i, 'adult_femalelice']\n",
    "        next_lice = sublice_data.loc[i + 1, 'adult_femalelice']\n",
    "        \n",
    "        state_idx = get_state_idx(current_lice, min_lice, dp_round)\n",
    "        next_state_idx = get_state_idx(next_lice, min_lice, dp_round)\n",
    "        \n",
    "        if is_training:\n",
    "            action = get_action(model, state_idx, eps)\n",
    "        else:\n",
    "            action = np.random.randint(0, model.num_actions)\n",
    "        \n",
    "        reward = calculate_reward(current_lice, next_lice, action, beta1, beta2, costs)\n",
    "        rewards.append(reward)\n",
    "        \n",
    "        if is_training:\n",
    "            model = update(model, state_idx, action, reward, next_state_idx, True)\n",
    "    \n",
    "    return model, sum(rewards)\n",
    "\n",
    "def run_experiment(model, epochs, locations, lice_data, eps, beta1, beta2, costs, dp_round, num_simulations=3):\n",
    "    q_learning_results = np.zeros((num_simulations, epochs))\n",
    "    random_results = np.zeros((num_simulations, epochs))\n",
    "    \n",
    "    for sim in range(num_simulations):\n",
    "        model.Q = np.zeros((model.num_states, model.num_actions))\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            q_learning_epoch_reward = 0\n",
    "            random_epoch_reward = 0\n",
    "            \n",
    "            for location in locations:\n",
    "                sublice_data = lice_data[lice_data['locality_number'] == location].reset_index(drop=True)\n",
    "                \n",
    "                if len(sublice_data) > 1:\n",
    "                    model, q_reward = simulate_episode(model, sublice_data, eps, beta1, beta2, costs, \n",
    "                                                    is_training=True, dp_round=dp_round)\n",
    "                    q_learning_epoch_reward += q_reward\n",
    "                    \n",
    "                    _, r_reward = simulate_episode(model, sublice_data, eps, beta1, beta2, costs, \n",
    "                                                is_training=False, dp_round=dp_round)\n",
    "                    random_epoch_reward += r_reward\n",
    "            \n",
    "            q_learning_results[sim, epoch] = q_learning_epoch_reward\n",
    "            random_results[sim, epoch] = random_epoch_reward\n",
    "    \n",
    "    return q_learning_results, random_results\n",
    "\n",
    "def plot_comparison(q_learning_results, random_results):\n",
    "    epochs = q_learning_results.shape[1]\n",
    "    epochs_range = np.arange(epochs)\n",
    "    \n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    q_mean = np.mean(q_learning_results, axis=0)\n",
    "    q_std = np.std(q_learning_results, axis=0)\n",
    "    random_mean = np.mean(random_results, axis=0)\n",
    "    random_std = np.std(random_results, axis=0)\n",
    "    \n",
    "    plt.plot(epochs_range, q_mean, 'b-', label='Q-Learning', linewidth=2)\n",
    "    plt.plot(epochs_range, random_mean, 'r--', label='Random Policy', linewidth=2)\n",
    "    plt.fill_between(epochs_range, q_mean - q_std, q_mean + q_std, color='b', alpha=0.2)\n",
    "    plt.fill_between(epochs_range, random_mean - random_std, random_mean + random_std, color='r', alpha=0.2)\n",
    "    \n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Total Reward')\n",
    "    plt.title('Q-Learning vs Random Policy: Total Rewards Over Time')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    print(f\"\\nFinal Statistics (over {len(q_learning_results)} simulations):\")\n",
    "    print(f\"Q-Learning - Final Mean Reward: {q_mean[-1]:.2f} ± {q_std[-1]:.2f}\")\n",
    "    print(f\"Random Policy - Final Mean Reward: {random_mean[-1]:.2f} ± {random_std[-1]:.2f}\")\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "#  parameters\n",
    "epochs = 30\n",
    "alpha = 0.5\n",
    "discount = 0.99  \n",
    "beta1 = 5\n",
    "beta2 = 0.5\n",
    "eps = 0.1\n",
    "costs = {True: 1, False: 0}\n",
    "num_simulations = 3\n",
    "\n",
    "# initialize model\n",
    "model = QLearning(num_states, num_actions, discount, alpha=alpha)\n",
    "\n",
    "# run experiment\n",
    "locations = set(lice_data['locality_number'])\n",
    "q_learning_results, random_results = run_experiment(\n",
    "    model, epochs, locations, lice_data, \n",
    "    eps, beta1, beta2, costs, dp_round, \n",
    "    num_simulations=num_simulations\n",
    ")\n",
    "\n",
    "# plot results\n",
    "plot_comparison(q_learning_results, random_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class QLearning:\n",
    "    def __init__(self, num_states, num_actions, discount, Q=None, alpha=0.5):\n",
    "        self.num_states = num_states\n",
    "        self.num_actions = num_actions  # Now 6 actions instead of 2\n",
    "        self.discount = discount\n",
    "        self.alpha = alpha\n",
    "        if Q is None or Q.size == 0:\n",
    "            self.Q = np.zeros((num_states, num_actions))\n",
    "        else:\n",
    "            self.Q = Q\n",
    "\n",
    "def get_state_idx(lice_level, min_lice, dp_round):\n",
    "    return int((lice_level - min_lice) * (10 ** dp_round))\n",
    "\n",
    "def get_action(model, state_idx, eps):\n",
    "    if np.random.random() < eps:\n",
    "        return np.random.randint(0, model.num_actions)\n",
    "    else:\n",
    "        return np.argmax(model.Q[state_idx])\n",
    "\n",
    "def get_action_from_data(row):\n",
    "    \"\"\"Convert treatment combinations to action index\n",
    "    0: no treatment\n",
    "    1: bath only\n",
    "    2: feed only\n",
    "    3: mechanical only\n",
    "    4: bath + feed\n",
    "    5: bath + mechanical\n",
    "    6: feed + mechanical\n",
    "    7: bath + feed + mechanical\n",
    "    \"\"\"\n",
    "    bath = row['bath_treatment']\n",
    "    feed = row['feed_treatment']\n",
    "    mech = row['mechanical_removal']\n",
    "    \n",
    "    if bath and feed and mech:\n",
    "        return 7\n",
    "    elif feed and mech:\n",
    "        return 6\n",
    "    elif bath and mech:\n",
    "        return 5\n",
    "    elif bath and feed:\n",
    "        return 4\n",
    "    elif mech:\n",
    "        return 3\n",
    "    elif feed:\n",
    "        return 2\n",
    "    elif bath:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0  # No treatment\n",
    "\n",
    "def get_action_cost(action):\n",
    "    \"\"\"Return cost of each action combination\n",
    "    Assuming: bath = 1, feed = 1.5, mechanical = 2 cost units\n",
    "    \"\"\"\n",
    "    costs = {\n",
    "        0: 0.0,    # no treatment\n",
    "        1: 1.0,    # bath only\n",
    "        2: 1.5,    # feed only\n",
    "        3: 2.0,    # mechanical only\n",
    "        4: 2.5,    # bath + feed\n",
    "        5: 3.0,    # bath + mechanical\n",
    "        6: 3.5,    # feed + mechanical\n",
    "        7: 4.5     # bath + feed + mechanical\n",
    "    }\n",
    "    return costs[action]\n",
    "\n",
    "def update(model, s, a, r, s2, model_large):\n",
    "    if 0 <= s < model.num_states and 0 <= s2 < model.num_states and 0 <= a < model.num_actions:\n",
    "        model.Q[s, a] += model.alpha * (r + model.discount * np.max(model.Q[s2, :]) - model.Q[s, a])\n",
    "    return model\n",
    "\n",
    "def calculate_reward(current_lice, next_lice, action, beta1, beta2, threshold=0.5):\n",
    "    # base penalty for current lice level\n",
    "    base_penalty = -beta1 * current_lice\n",
    "    \n",
    "    # extra penalty for exceeding 0.5 \n",
    "    threshold_penalty = -10 * beta1 * max(0, current_lice - threshold)\n",
    "    \n",
    "    # reward for reducing lice levels\n",
    "    reduction_reward = 5 * beta1 * max(0, current_lice - next_lice)\n",
    "    \n",
    "    # treatment cost using expanded action costs\n",
    "    treatment_cost = -beta2 * get_action_cost(action)\n",
    "    \n",
    "    return base_penalty + threshold_penalty + reduction_reward + treatment_cost\n",
    "\n",
    "def simulate_episode(model, sublice_data, eps, beta1, beta2, is_training=True, dp_round=2):\n",
    "    rewards = []\n",
    "    min_lice = sublice_data['adult_femalelice'].min()\n",
    "    \n",
    "    for i in range(len(sublice_data) - 1):\n",
    "        current_lice = sublice_data.loc[i, 'adult_femalelice']\n",
    "        next_lice = sublice_data.loc[i + 1, 'adult_femalelice']\n",
    "        \n",
    "        state_idx = get_state_idx(current_lice, min_lice, dp_round)\n",
    "        next_state_idx = get_state_idx(next_lice, min_lice, dp_round)\n",
    "        \n",
    "        if is_training:\n",
    "            action = get_action(model, state_idx, eps)\n",
    "        else:\n",
    "            action = np.random.randint(0, model.num_actions)\n",
    "        \n",
    "        reward = calculate_reward(current_lice, next_lice, action, beta1, beta2)\n",
    "        rewards.append(reward)\n",
    "        \n",
    "        if is_training:\n",
    "            model = update(model, state_idx, action, reward, next_state_idx, True)\n",
    "    \n",
    "    return model, sum(rewards)\n",
    "\n",
    "def run_experiment(model, epochs, locations, lice_data, eps, beta1, beta2, dp_round, num_simulations=3):\n",
    "    q_learning_results = np.zeros((num_simulations, epochs))\n",
    "    random_results = np.zeros((num_simulations, epochs))\n",
    "    \n",
    "    for sim in range(num_simulations):\n",
    "        model.Q = np.zeros((model.num_states, model.num_actions))\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            q_learning_epoch_reward = 0\n",
    "            random_epoch_reward = 0\n",
    "            \n",
    "            for location in locations:\n",
    "                sublice_data = lice_data[lice_data['locality_number'] == location].reset_index(drop=True)\n",
    "                \n",
    "                if len(sublice_data) > 1:\n",
    "                    model, q_reward = simulate_episode(model, sublice_data, eps, beta1, beta2, \n",
    "                                                    is_training=True, dp_round=dp_round)\n",
    "                    q_learning_epoch_reward += q_reward\n",
    "                    \n",
    "                    _, r_reward = simulate_episode(model, sublice_data, eps, beta1, beta2, \n",
    "                                                is_training=False, dp_round=dp_round)\n",
    "                    random_epoch_reward += r_reward\n",
    "            \n",
    "            q_learning_results[sim, epoch] = q_learning_epoch_reward\n",
    "            random_results[sim, epoch] = random_epoch_reward\n",
    "    \n",
    "    return q_learning_results, random_results\n",
    "\n",
    "# parameters\n",
    "epochs = 10\n",
    "alpha = 0.5\n",
    "discount = 0.99\n",
    "beta1 = 5\n",
    "beta2 = 0.5\n",
    "eps = 0.1\n",
    "num_simulations = 3\n",
    "num_actions = 8  \n",
    "\n",
    "# initialize model \n",
    "model = QLearning(num_states, num_actions, discount, alpha=alpha)\n",
    "\n",
    "# run experiment\n",
    "locations = set(lice_data['locality_number'])\n",
    "q_learning_results, random_results = run_experiment(\n",
    "    model, epochs, locations, lice_data, \n",
    "    eps, beta1, beta2, dp_round, \n",
    "    num_simulations=num_simulations\n",
    ")\n",
    "\n",
    "plot_comparison(q_learning_results, random_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
